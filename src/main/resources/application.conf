#pekko.http.server.idle-timeout
#pekko.http.client.idle-timeout
#pekko.http.client.http2.completion-timeout = 6s

pekko {
  loggers = ["org.apache.pekko.event.slf4j.Slf4jLogger"]
  logger-startup-timeout = 30s
  logging-filter = "org.apache.pekko.event.slf4j.Slf4jLoggingFilter"
  loglevel = DEBUG
  log-dead-letters = on
  log-dead-letters-during-shutdown = false

  http {
    server {
      preview.enable-http2 = on

      # The time after which an idle connection will be automatically closed.
      # Set to `infinite` to completely disable idle connection timeouts.
      idle-timeout = 60s
    }

    client {
      #idle-timeout = 60s
      http2 {
        completion-timeout = 5s
      }
    }
  }

  actor {
    provider = cluster

    serializers {

      #pekko-containers = "org.apache.pekko.remote.serialization.MessageContainerSerializer"
      #pekko-misc = "org.apache.pekko.remote.serialization.MiscMessageSerializer"
      #artery = "org.apache.pekko.remote.serialization.ArteryMessageSerializer"
      #daemon-create = "org.apache.pekko.remote.serialization.DaemonMsgCreateSerializer"
      #pekko-system-msg = "org.apache.pekko.remote.serialization.SystemMessageSerializer"

      #ser = server.grpc.serialization.DomainSerializer

      proto = "org.apache.pekko.remote.serialization.ProtobufSerializer" //id = 2

      //proto = "org.apache.pekko.stream.serialization.ProtobufSerializer2"

      //org.apache.pekko.stream.serialization.StreamRefSerializerBBufer replaces org.apache.pekko.stream.serialization.StreamRefSerializer
      pekko-stream-ref = "org.apache.pekko.stream.serialization.StreamRefSerializerBBufer"

    }

    serialization-bindings {

      "scalapb.GeneratedMessage" = proto

      #"com.domain.chat.ChatCmd" = ser
      #"com.domain.user.UserTwinCmd" = ser
      #"com.domain.chat.ChatReply" = ser
      #"server.grpc.chat.ClientCmd" = ser
      #"server.grpc.chat.ServerCmd" = ser
    }

    serialization-identifiers {
      #"org.apache.pekko.remote.serialization.ProtobufSerializer2" = 2
      "pekko.stream.serialization.StreamRefSerializer" = 30
    }


    # Default separate internal dispatcher to run Akka internal tasks and actors on
    # protecting them against starvation because of accidental blocking in user actors (which run on the
    # default dispatcher)
    internal-dispatcher {
      type = "Dispatcher"
      executor = "fork-join-executor"
      throughput = 5
      fork-join-executor {
        parallelism-factor = 1.0
        parallelism-min = 2
        parallelism-max = 6
      }
    }

    #this dispatcher uses up to 4 threads
    default-dispatcher {
      fork-join-executor {
        parallelism-factor = 1.0
        parallelism-min = 2
        parallelism-max = 6
      }
    }
  }

  remote {
    # Disable event logging
    classic.log-remote-lifecycle-events = off
    artery {
      # Select the underlying transport implementation.
      # Possible values: aeron-udp, tcp, tls-tcp
      # See https://doc.akka.io/docs/akka/current/remoting-artery.html#selecting-a-transport for the tradeoffs
      # for each transport

      #https://doc.akka.io/docs/akka/current/remoting-artery.html#selecting-a-transport
      transport = aeron-udp

      #transport = tls-tcp
      #ssl.config-ssl-engine {
      #key-store = "./ks/safer-chat.jks"
      #trust-store = "./ks/safer-chat.jks"
      #key-store-password = ${SSL_KEY_STORE_PASSWORD}
      #key-password = ${SSL_KEY_PASSWORD}
      #trust-store-password = ${SSL_TRUST_STORE_PASSWORD}
      #protocol = "TLSv1.2"
      #enabled-algorithms = [TLS_DHE_RSA_WITH_AES_128_GCM_SHA256]
      #}

      large-message-destinations = []

      # To notice large messages you can enable logging of message types with payload size in bytes larger than the configured
      log-frame-size-exceeding = 62KiB

      advanced {
        # Maximum serialized message size, including header data.
        maximum-frame-size = 64KiB

        # Direct byte buffers are reused in a pool with this maximum size.
        # Each buffer has the size of 'maximum-frame-size'.
        # This is not a hard upper limit on number of created buffers. Additional
        # buffers will be created if needed, e.g. when using many outbound
        # associations at the same time. Such additional buffers will be garbage
        # collected, which is not as efficient as reusing buffers in the pool.
        buffer-pool-size = 128

        # Maximum serialized message size for the large messages, including header data.
        # It is currently restricted to 1/8th the size of a term buffer that can be
        # configured by setting the 'aeron.term.buffer.length' system property.
        # See 'large-message-destinations'.
        maximum-large-frame-size = 2 MiB

        # Direct byte buffers for the large messages are reused in a pool with this maximum size.
        # Each buffer has the size of 'maximum-large-frame-size'.
        # See 'large-message-destinations'.
        # This is not a hard upper limit on number of created buffers. Additional
        # buffers will be created if needed, e.g. when using many outbound
        # associations at the same time. Such additional buffers will be garbage
        # collected, which is not as efficient as reusing buffers in the pool.
        large-buffer-pool-size = 32


        # Total number of inbound lanes, shared among all inbound associations. A value
        # greater than 1 means that deserialization can be performed in parallel for
        # different destination actors. The selection of lane is based on consistent
        # hashing of the recipient ActorRef to preserve message ordering per receiver.
        # Lowest latency can be achieved with inbound-lanes=1 because of one less
        # asynchronous boundary.
        inbound-lanes = 4

        # Number of outbound lanes for each outbound association. A value greater than 1
        # means that serialization and other work can be performed in parallel for different
        # destination actors. The selection of lane is based on consistent hashing of the
        # recipient ActorRef to preserve message ordering per receiver. Note that messages
        # for different destination systems (hosts) are handled by different streams also
        # when outbound-lanes=1. Lowest latency can be achieved with outbound-lanes=1
        # because of one less asynchronous boundary.
        outbound-lanes = 1


        # Only used when transport is aeron-udp
        aeron {

          # Periodically log out all Aeron counters. See https://github.com/real-logic/aeron/wiki/Monitoring-and-Debugging#counters
          # Only used when transport is aeron-udp.
          log-aeron-counters = false

          # Controls whether to start the Aeron media driver in the same JVM or use external
          # process. Set to 'off' when using external media driver, and then also set the
          # 'aeron-dir'.
          # Only used when transport is aeron-udp.
          embedded-media-driver = on

          # Directory used by the Aeron media driver. It's mandatory to define the 'aeron-dir'
          # if using external media driver, i.e. when 'embedded-media-driver = off'.
          # Embedded media driver will use a this directory, or a temporary directory if this
          # property is not defined (empty).
          # Only used when transport is aeron-udp.
          #aeron-dir = ""

          # Whether to delete aeron embedded driver directory upon driver stop.
          # Only used when transport is aeron-udp.
          delete-aeron-dir = yes

          # Level of CPU time used, on a scale between 1 and 10, during backoff/idle.
          # The tradeoff is that to have low latency more CPU time must be used to be
          # able to react quickly on incoming messages or send as fast as possible after
          # backoff backpressure.
          # Level 1 strongly prefer low CPU consumption over low latency.
          # Level 10 strongly prefer low latency over low CPU consumption.
          # Only used when transport is aeron-udp.
          idle-cpu-level = 1

          # messages that are not accepted by Aeron are dropped after retrying for this period
          # Only used when transport is aeron-udp.
          give-up-message-after = 60 seconds

          # Timeout after which aeron driver has not had keepalive messages
          # from a client before it considers the client dead.
          # Only used when transport is aeron-udp.
          client-liveness-timeout = 20 seconds

          # Timout after after which an uncommitted publication will be unblocked
          # Only used when transport is aeron-udp.
          publication-unblock-timeout = 40 seconds

          # Timeout for each the INACTIVE and LINGER stages an aeron image
          # will be retained for when it is no longer referenced.
          # This timeout must be less than the 'handshake-timeout'.
          # Only used when transport is aeron-udp.
          image-liveness-timeout = 10 seconds

          # Timeout after which the aeron driver is considered dead
          # if it does not update its C'n'C timestamp.
          # Only used when transport is aeron-udp.
          driver-timeout = 20 seconds
        }
      }
    }
  }

  cluster {
    downing-provider-class = "org.apache.pekko.cluster.sbr.SplitBrainResolverProvider"

    app-version = ${APP_VERSION}

    # Expected failover time:
    #
    # There are several configured timeouts that add to the total failover latency. With default configuration those are:
    #
    # 1) failure detection 5 seconds
    # 2) stable-after 7 seconds
    # 3) akka.cluster.down-removal-margin (by default the same as split-brain-resolver.stable-after) 7 seconds
    #
    # In total, you can expect the failover time of a singleton or sharded instance to be around (5+7+(7*3/4) = 18) seconds with this configuration.
    #
    split-brain-resolver {

      # static-quorum, keep-majority, keep-oldest, down-all, lease-majority
      # active-strategy = keep-majority
      active-strategy = keep-majority

      #//#stable-after
      # Time margin after which shards or singletons that belonged to a downed/removed
      # partition are created in surviving partition. The purpose of this margin is that
      # in case of a network partition the persistent actors in the non-surviving partitions
      # must be stopped before corresponding persistent actors are started somewhere else.
      # This is useful if you implement downing strategies that handle network partitions,
      # e.g. by keeping the larger side of the partition and shutting down the smaller side.
      # Decision is taken by the strategy when there has been no membership or
      # reachability changes for this duration, i.e. the cluster state is stable.
      stable-after = 7 s

      # When reachability observations by the failure detector are changed the SBR decisions
      # are deferred until there are no changes within the 'stable-after' duration.
      # If this continues for too long it might be an indication of an unstable system/network
      # and it could result in delayed or conflicting decisions on separate sides of a network
      # partition.
      # As a precaution for that scenario all nodes are downed if no decision is made within
      # `stable-after + down-all-when-unstable` from the first unreachability event.
      # The measurement is reset if all unreachable have been healed, downed or removed, or
      # if there are no changes within `stable-after * 2`.
      # The value can be on, off, or a duration.
      # By default it is 'on' and then it is derived to be 3/4 of stable-after.
      down-all-when-unstable = off
    }

    sharding {
      # Rebalance check is performed periodically with this interval
      rebalance-interval = 30s
      number-of-shards = 256

      # Config path of the lease that each shard must acquire before starting entity actors
      # default is no lease
      # A lease can also be used for the singleton coordinator by settings it in the coordinator-singleton properties
      #use-lease = pekko.coordination.lease.cassandra
    }

  }

  coordinated-shutdown {

    # Exit the JVM (System.exit(0)) in the last phase actor-system-terminate
    # if this is set to 'on'. It is done after termination of the
    # ActorSystem if terminate-actor-system=on, otherwise it is done
    # immediately when the last phase is reached.
    exit-jvm = on

    default-phase-timeout = 4 seconds

    phases {
      # Wait until exiting has been completed
      cluster-exiting-done {
        timeout = 12 s # increase if to many chatrooms need to be released.
        depends-on = [cluster-exiting]
      }
    }
  }

  stream.materializer {
    initial-input-buffer-size = ${safer-chat.buffer-size}
    max-input-buffer-size = ${safer-chat.buffer-size}
    max-fixed-buffer-size = ${safer-chat.buffer-size}
    dispatcher = pekko.actor.default-dispatcher #stream-dispatcher

    stream-ref {
      buffer-capacity = ${safer-chat.buffer-size}
      subscription-timeout = 3 seconds
    }
  }

  persistence {
    state.plugin = cassandra
  }

}

pekko.extensions = ["org.apache.pekko.cluster.metrics.ClusterMetricsExtension"]

pekko.management.http {
  port = 8558
}


pekko.management.cluster.bootstrap.contact-point-discovery {
  service-name = safer-chat
  discovery-method = config

  # boostrap filters ports with the same IP assuming they are previous instances running on the same node
  # unless a port is specified
  port-name = "management"
  required-contact-point-nr = 1
  # config service discovery never changes
  stable-margin = 3 ms
  # bootstrap without all the nodes being up
  contact-with-all-contact-points = false
}

pekko.discovery.config.services {
  "safer-chat" {
    endpoints = [
      {host = "127.0.0.1", port = 8558}
      {host = "127.0.0.2", port = 8558}
    ]
  }
}

#TODO
#pekko.connectors.cassandra.session-provider

cassandra {
  keyspace = "chat"

  username = "..."
  psw = "..."
  class = org.apache.pekko.cassandra.CassandraStore
  parallelism = 4
  max-batch-size = 8
}


safer-chat {
  grpc-port = 8080
  grpc-port = ${?PORT}

  http-port = 8443

  banned-users = [aa, bb, cc]

  banned-hosts = [127.0.0.2, 127.0.0.3]

  buffer-size = 16

  secret-token = "1ยง9+_CE_GJ01QdD7TcMFtA6QpMaVA1spl1q#))z_13fsfl,;`xJkHKBvxLweO9*()*)U&&^*&*bdd)o?>"

  #To read historical messages
  default = "ojViMp1jBCE_GJ01QdD7TcMFtA6QpMaVA1splBEoD_k"
}

pekko.grpc.client {
  "server.grpc.ChatRoom" {
    host = ${?SERVER_HOSTNAME}
    host = 127.0.0.1
    port = 8080

    override-authority = codelfsolutions.com
    trusted = /fsa/fullchain.pem


    #override-authority = "localhost"
    #use-tls = false
  }
}
